# -*- coding: utf-8 -*-
"""Copia_de_Regresión_clase_autos_Angel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/AngelFelixV/Regresi-n_predictor_autos/blob/patch-1/Copia_de_Regresi%C3%B3n_clase_autos_Angel.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
import random
import math
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import linear_model
import numpy as np
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

df_raw = pd.read_csv('/content/car_price_prediction (1).csv', header=0)
df_raw

df_raw['Category'].value_counts()

df_raw.isnull().sum()

df_raw.isna().sum()

df_raw.info()

def contador_de_datos(df, columnas, titles=None, max_cols=2):
    n = len(columnas)
    rows = (n + max_cols - 1) // max_cols

    plt.figure(figsize=(5 * max_cols, 4 * rows))

    for i, col in enumerate(columnas):
        plt.subplot(rows, max_cols, i + 1)
        sns.countplot(data=df, x=col, palette='pastel', order=df[col].value_counts().index)
        plt.title(titles[i] if titles else col)
        plt.xticks(rotation=45)
        plt.xlabel("")
        plt.ylabel("Count")

    plt.tight_layout()
    plt.suptitle("Distributions of Applicants", fontsize=16, y=1.02)
    plt.show()

columnas=['Prod. year','Category','Leather interior','Fuel type','Cylinders','Gear box type','Drive wheels','Color','Doors','Airbags']
contador_de_datos(df_raw, columnas)

df_raw.drop(columns=['ID'],inplace=True)

df_raw['Levy'] = df_raw['Levy'].astype(str)
df_raw['Levy']=df_raw['Levy'].str.replace('-','0')

df_raw['Levy']=df_raw['Levy'].astype(float)

df_raw.head()

df_raw['Mileage'] = df_raw['Mileage'].astype(str)
df_raw['Mileage'] = df_raw['Mileage'].str.replace(' km','')

df_raw['Mileage'] = df_raw['Mileage'].astype(float)

df_raw['Doors'] = df_raw['Doors'].str.extract(r'(\d+)').astype(int)

df_raw

df_raw['Turbo_status'] = np.where(df_raw['Engine volume'].str.contains('Turbo', case=False, na=False), 'Turbo', 'No turbo')
df_raw['Engine volume'] = df_raw['Engine volume'].str.extract(r'(\d+\.?\d*)')[0].astype(float)

df_raw

plt.boxplot(df_raw['Price'])
plt.title('Boxplot de Price')
plt.ylabel('Valor')
plt.show()

plt.boxplot(df_raw['Levy'])
plt.title('Boxplot de Levy')
plt.ylabel('Valor')
plt.show()

plt.boxplot(df_raw['Mileage'])
plt.title('Boxplot de Mileage')
plt.ylabel('Valor')
plt.show()

def quitar_outliers(df, variable):
    Q1 = df[variable].quantile(0.25)
    Q3 = df[variable].quantile(0.75)
    IQR = Q3 - Q1

    limite_inferior = Q1 - 1.5*IQR
    limite_superior = Q3 + 1.5*IQR

    df_filtrado = df[(df[variable] >= limite_inferior) & (df[variable] <= limite_superior)]

    return df_filtrado

def quitar_outliers_std(df, variable):
    mean_val = df[variable].mean()
    std_dev = df[variable].std()

    # Define bounds for outliers (e.g., 3 standard deviations from the mean)
    limite_inferior = mean_val - 3 * std_dev
    limite_superior = mean_val + 3 * std_dev

    df_filtrado = df[(df[variable] >= limite_inferior) & (df[variable] <= limite_superior)]

    return df_filtrado

# Apply the new outlier removal function to df_raw
df_raw_std_filtered = df_raw.copy()
df_raw_std_filtered = quitar_outliers_std(df_raw_std_filtered, 'Price')
df_raw_std_filtered = quitar_outliers_std(df_raw_std_filtered, 'Levy')
df_raw_std_filtered = quitar_outliers_std(df_raw_std_filtered, 'Mileage')

print(f"Original DataFrame shape: {df_raw.shape}")
print(f"Filtered DataFrame shape (mean/std method): {df_raw_std_filtered.shape}")
display(df_raw_std_filtered.head())

quitar_outliers(df_raw, 'Price')
quitar_outliers(df_raw, 'Levy')
quitar_outliers(df_raw, 'Mileage')

plt.boxplot(df_raw_std_filtered['Price'])
plt.title('Boxplot de Price')
plt.ylabel('Valor')
plt.show()

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse_output=False)

# Create a copy to avoid modifying the original filtered DataFrame directly
df_for_encoding = df_raw_std_filtered[['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color','Turbo_status']].copy()

# Convert 'Prod. year' to string before fitting the encoder
df_for_encoding['Prod. year'] = df_for_encoding['Prod. year'].astype(str)

X_encoded = encoder.fit_transform(df_for_encoding)
encoded_cols = encoder.get_feature_names_out(['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color','Turbo_status'])
X_encoded = pd.DataFrame(X_encoded, columns=encoded_cols, index=df_raw_std_filtered.index)

df = pd.concat([df_raw_std_filtered.drop(columns=['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color','Turbo_status']), X_encoded], axis=1)

df

from sklearn.preprocessing import StandardScaler
X = df.drop(columns='Price')
y = df['Price']
scaler = StandardScaler()
X_nor = scaler.fit_transform(X)

y_scaler = StandardScaler()
y_nor = y_scaler.fit_transform(y.values.reshape(-1,1))

X_final = pd.DataFrame(X_nor, columns = X.columns)
y_final = pd.DataFrame(y_nor, columns = ['Price'])

X_final

y_final

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)

regresion_multi_var = linear_model.LinearRegression()
regresion_multi_var.fit(X_train,y_train)

from sklearn.metrics import mean_squared_error
y_pred = regresion_multi_var.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test,y_pred))
print(rmse)

r2 = regresion_multi_var.score(X_test,y_test)
print('coeficiente de indeterminación R2: {r2}'.format(r2 = r2))

df = pd.concat([df_raw_std_filtered.drop(columns=['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color','Turbo_status']), X_encoded], axis=1)

df

from sklearn.preprocessing import StandardScaler
X = df.drop(columns='Price')
y = df['Price']
scaler = StandardScaler()
X_nor = scaler.fit_transform(X)

y_scaler = StandardScaler()
y_nor = y_scaler.fit_transform(y.values.reshape(-1,1))

X_final = pd.DataFrame(X_nor, columns = X.columns)
y_final = pd.DataFrame(y_nor, columns = ['Price'])

X_final

y_final

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)

regresion_multi_var = linear_model.LinearRegression()
regresion_multi_var.fit(X_train,y_train)

from sklearn.metrics import mean_squared_error
y_pred = regresion_multi_var.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test,y_pred))
print(rmse)

r2 = regresion_multi_var.score(X_test,y_test)
print('coeficiente de indeterminación R2: {r2}'.format(r2 = r2))

def predict_new_car_price(car_data):
    """
    Predicts the price of a car based on its features using the trained linear regression model.

    Args:
        car_data (dict): A dictionary containing the features of the car.
                         Example: {'Levy': '1399', 'Manufacturer': 'LEXUS', 'Model': 'RX 450',
                                   'Prod. year': 2010, 'Category': 'Jeep', 'Leather interior': 'Yes',
                                   'Fuel type': 'Hybrid', 'Engine volume': '3.5', 'Mileage': '186005 km',
                                   'Cylinders': 6.0, 'Gear box type': 'Automatic', 'Drive wheels': '4x4',
                                   'Doors': '04-May', 'Wheel': 'Left wheel', 'Color': 'Silver', 'Airbags': 12}

    Returns:
        float: The predicted price of the car.
    """

    # Convert input dictionary to a pandas DataFrame (single row)
    input_df = pd.DataFrame([car_data])

    # Apply the same preprocessing steps as the training data

    # 1. Handle 'Levy' column
    input_df['Levy'] = input_df['Levy'].astype(str)
    input_df['Levy'] = input_df['Levy'].str.replace('-', '0')
    input_df['Levy'] = input_df['Levy'].astype(float)

    # 2. Handle 'Mileage' column
    input_df['Mileage'] = input_df['Mileage'].astype(str)
    input_df['Mileage'] = input_df['Mileage'].str.replace(' km', '')
    input_df['Mileage'] = input_df['Mileage'].astype(float)

    # 3. Handle 'Doors' column
    # Ensure the string extraction for 'Doors' is robust
    input_df['Doors'] = input_df['Doors'].str.extract(r'(\d+)').astype(int)

    # 4. Handle 'Engine volume' and create 'Turbo_status'
    input_df['Turbo_status'] = np.where(input_df['Engine volume'].str.contains('Turbo', case=False, na=False), 'Turbo', 'No turbo')
    input_df['Engine volume'] = input_df['Engine volume'].astype(str).str.extract(r'(\d+\.?\d*)')[0].astype(float)

    # 5. One-hot encode categorical features using the fitted encoder
    categorical_cols = ['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color', 'Turbo_status']
    # For 'Prod. year', ensure it's treated as string for encoding if it was during fit
    input_df['Prod. year'] = input_df['Prod. year'].astype(str)

    # Set handle_unknown='error' to raise a ValueError for unknown categories
    encoded_features = encoder.transform(input_df[categorical_cols])
    encoded_features_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols), index=input_df.index)

    # 6. Select numerical features and concatenate with encoded features
    numerical_cols = ['Levy', 'Engine volume', 'Mileage', 'Cylinders', 'Doors', 'Airbags']
    processed_input_df = pd.concat([input_df[numerical_cols], encoded_features_df], axis=1)

    # 7. Reindex to match the training data columns exactly and fill missing with 0
    #    This handles cases where a new car might have a category not seen during training
    #    or if the order of columns is different.
    processed_input_df = processed_input_df.reindex(columns=X.columns, fill_value=0)

    # 8. Scale all features using the fitted scaler
    scaled_features = scaler.transform(processed_input_df)
    scaled_features_df = pd.DataFrame(scaled_features, columns=X.columns, index=processed_input_df.index)

    # 9. Make prediction using the trained model
    scaled_prediction = regresion_multi_var.predict(scaled_features_df)

    # 10. Inverse transform the scaled prediction to get the original price scale
    original_price = y_scaler.inverse_transform(scaled_prediction.reshape(-1, 1))[0][0]

    return original_price

sample_car_with_unknown_manufacturer = {
    'Levy': '1399', 'Manufacturer': 'UNKNOWN_MANUFACTURER', 'Model': 'RX 450', 'Prod. year': 2010,
    'Category': 'Jeep', 'Leather interior': 'Yes', 'Fuel type': 'Hybrid', 'Engine volume': '3.5',
    'Mileage': '186005 km', 'Cylinders': 6.0, 'Gear box type': 'Automatic', 'Drive wheels': '4x4',
    'Doors': '04-May', 'Wheel': 'Left wheel', 'Color': 'Silver', 'Airbags': 12
}

try:
    predicted_price = predict_new_car_price(sample_car_with_unknown_manufacturer)
    print(f"Predicted Price: {predicted_price}")
except ValueError as e:
    print("Error")

"""# Task
Compute the mode for each categorical column and the mean for each numerical column across the `df_raw_std_filtered` dataset. These statistics will serve as the initial default values in the Streamlit application.

## Calculate_Global_Feature_Statistics

### Subtask:
Compute the mode for each categorical column and the mean for each numerical column across the entire `df_raw_std_filtered` dataset. These statistics will serve as the initial default values in the Streamlit application.

**Reasoning**:
To compute the mode for each categorical column and the mean for each numerical column across the entire `df_raw_std_filtered` dataset, I will first explicitly define the numerical and categorical columns as specified in the instructions. Then, I will calculate the mean for numerical columns and the mode for categorical columns, handling cases with multiple modes by selecting the first one. Finally, I'll combine these into a single dictionary.
"""

numerical_cols_for_stats = ['Levy', 'Engine volume', 'Mileage', 'Cylinders', 'Doors', 'Airbags']
categorical_cols_for_stats = ['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color', 'Turbo_status']

# Calculate means for numerical columns
mean_values = df_raw_std_filtered[numerical_cols_for_stats].mean().to_dict()

# Calculate modes for categorical columns
mode_values = {}
for col in categorical_cols_for_stats:
    mode_values[col] = df_raw_std_filtered[col].mode()[0] # Get the first mode if there are multiple

# Combine all statistics into one dictionary
global_default_values = {**mean_values, **mode_values}

print("Global Default Values:")
for k, v in global_default_values.items():
    print(f"{k}: {v}")

"""## Calculate_Model_Specific_Feature_Statistics

### Subtask:
Create a dictionary or similar structure that maps each unique 'Model' to the mode (for categorical features) or mean (for numerical features) of all other car attributes observed when that particular 'Model' is present in the `df_raw_std_filtered` dataset. This will be used to provide dynamic default suggestions in the Streamlit app.

**Reasoning**:
To calculate the model-specific statistics, I will iterate through each unique model in `df_raw_std_filtered`, filter the DataFrame for that model, and then compute the mean for numerical columns and the mode for categorical columns, storing these in a nested dictionary.
"""

model_specific_defaults = {}
unique_models = df_raw_std_filtered['Model'].unique()

for model in unique_models:
    # Filter the DataFrame for the current model
    model_df = df_raw_std_filtered[df_raw_std_filtered['Model'] == model]

    # Calculate means for numerical columns
    model_mean_values = model_df[numerical_cols_for_stats].mean().to_dict()

    # Calculate modes for categorical columns
    model_mode_values = {}
    for col in categorical_cols_for_stats:
        # Ensure 'Prod. year' is treated as string for mode calculation if it was during encoding
        if col == 'Prod. year':
            model_mode_values[col] = model_df[col].astype(str).mode()[0]
        else:
            model_mode_values[col] = model_df[col].mode()[0]

    # Combine numerical and categorical stats for the current model
    model_specific_defaults[model] = {**model_mean_values, **model_mode_values}

# Display a few examples from the created dictionary to verify
print("Model-Specific Default Values (first 3 models):")
for i, (model_name, defaults) in enumerate(model_specific_defaults.items()):
    if i >= 3: # Limit output to first 3 for brevity
        break
    print(f"\nModel: {model_name}")
    for k, v in defaults.items():
        print(f"  {k}: {v}")

"""## Update_Predict_Function

### Subtask:
Review the existing `predict_new_car_price` function. As the Streamlit app will be designed to ensure all necessary inputs are provided (either by user selection or dynamic defaults), the prediction function itself will not require modifications to handle missing inputs. This step is to confirm its readiness.

**Reasoning**:
To review the `predict_new_car_price` function as requested, I will first display the function's code to carefully examine its implementation against the provided instructions.
"""

def predict_new_car_price(car_data):
    """
    Predicts the price of a car based on its features using the trained linear regression model.

    Args:
        car_data (dict): A dictionary containing the features of the car.
                         Example: {'Levy': '1399', 'Manufacturer': 'LEXUS', 'Model': 'RX 450',
                                   'Prod. year': 2010, 'Category': 'Jeep', 'Leather interior': 'Yes',
                                   'Fuel type': 'Hybrid', 'Engine volume': '3.5', 'Mileage': '186005 km',
                                   'Cylinders': 6.0, 'Gear box type': 'Automatic', 'Drive wheels': '4x4',
                                   'Doors': '04-May', 'Wheel': 'Left wheel', 'Color': 'Silver', 'Airbags': 12}

    Returns:
        float: The predicted price of the car.
    """

    # Convert input dictionary to a pandas DataFrame (single row)
    input_df = pd.DataFrame([car_data])

    # Apply the same preprocessing steps as the training data

    # 1. Handle 'Levy' column
    input_df['Levy'] = input_df['Levy'].astype(str)
    input_df['Levy'] = input_df['Levy'].str.replace('-', '0')
    input_df['Levy'] = input_df['Levy'].astype(float)

    # 2. Handle 'Mileage' column
    input_df['Mileage'] = input_df['Mileage'].astype(str)
    input_df['Mileage'] = input_df['Mileage'].str.replace(' km', '')
    input_df['Mileage'] = input_df['Mileage'].astype(float)

    # 3. Handle 'Doors' column
    # Ensure the string extraction for 'Doors' is robust
    input_df['Doors'] = input_df['Doors'].str.extract(r'(\d+)')[0].astype(int)

    # 4. Handle 'Engine volume' and create 'Turbo_status'
    input_df['Turbo_status'] = np.where(input_df['Engine volume'].astype(str).str.contains('Turbo', case=False, na=False), 'Turbo', 'No turbo')
    input_df['Engine volume'] = input_df['Engine volume'].astype(str).str.extract(r'(\d+\.?\d*)')[0].astype(float)

    # 5. One-hot encode categorical features using the fitted encoder
    categorical_cols = ['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color', 'Turbo_status']
    # For 'Prod. year', ensure it's treated as string for encoding if it was during fit
    input_df['Prod. year'] = input_df['Prod. year'].astype(str)

    # Set handle_unknown='error' to raise a ValueError for unknown categories
    encoded_features = encoder.transform(input_df[categorical_cols])
    encoded_features_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols), index=input_df.index)

    # 6. Select numerical features and concatenate with encoded features
    numerical_cols = ['Levy', 'Engine volume', 'Mileage', 'Cylinders', 'Doors', 'Airbags']
    processed_input_df = pd.concat([input_df[numerical_cols], encoded_features_df], axis=1)

    # 7. Reindex to match the training data columns exactly and fill missing with 0
    #    This handles cases where a new car might have a category not seen during training
    #    or if the order of columns is different.
    processed_input_df = processed_input_df.reindex(columns=X.columns, fill_value=0)

    # 8. Scale all features using the fitted scaler
    scaled_features = scaler.transform(processed_input_df)
    scaled_features_df = pd.DataFrame(scaled_features, columns=X.columns, index=processed_input_df.index)

    # 9. Make prediction using the trained model
    scaled_prediction = regresion_multi_var.predict(scaled_features_df)

    # 10. Inverse transform the scaled prediction to get the original price scale
    original_price = y_scaler.inverse_transform(scaled_prediction.reshape(-1, 1))[0][0]

    return original_price

print("The predict_new_car_price function is defined and ready for examination.")

"""## Implement_Dynamic_Streamlit_App

### Subtask:
Develop the Streamlit application using the calculated global and model-specific statistics. The app will feature input widgets for `Manufacturer`, `Model`, `Prod. year`, `Color`, `Gear box type`, `Fuel type` as explicitly requested, and other features. It will use `st.session_state` and callback functions to dynamically update default values for other features when a `Model` is selected. Finally, it will call the `predict_new_car_price` function with the collected and defaulted inputs to display the predicted price.

## Summary:

### Data Analysis Key Findings

*   **Global Default Values Computed**:
    *   For numerical features, the following mean values were calculated across the dataset: `Levy`: 612.08, `Engine volume`: 2.29, `Mileage`: 166931.02, `Cylinders`: 4.56, `Doors`: 3.93, `Airbags`: 6.58.
    *   For categorical features, the following mode values were identified: `Manufacturer`: HYUNDAI, `Model`: Prius, `Prod. year`: 2012, `Category`: Sedan, `Leather interior`: Yes, `Fuel type`: Petrol, `Gear box type`: Automatic, `Drive wheels`: Front, `Wheel`: Left wheel, `Color`: Black, `Turbo_status`: No turbo.
    *   These global statistics are consolidated into `global_default_values` for initial app defaults.
*   **Model-Specific Default Values Generated**: A `model_specific_defaults` dictionary was successfully created. This dictionary maps each unique car model to its own set of default statistics (mean for numerical features, mode for categorical features), which will enable dynamic default suggestions within the Streamlit application based on the selected car model.
*   **Prediction Function Readiness Confirmed**: The `predict_new_car_price` function was reviewed and confirmed to be robust and ready for use. It incorporates all necessary preprocessing steps (e.g., handling `Levy`, `Mileage`, `Doors`, `Engine volume`, one-hot encoding, scaling, and reindexing) to accurately predict car prices given a set of input features.

### Insights or Next Steps

*   The pre-calculated global and model-specific statistics provide a solid foundation for building the Streamlit application, allowing for intelligent default values that enhance user experience.
*   The next immediate step is to develop the Streamlit application, integrating these default statistics and the `predict_new_car_price` function to create a dynamic and functional car price prediction interface.
"""



"""# Task
Calculate the critical t-value for a 95% prediction interval using the degrees of freedom from the training data, and integrate this into the `predict_new_car_price` function to return both the point prediction and its 95% prediction interval.

## Calculate_T_Critical_Value

### Subtask:
Calculate the critical t-value for a 95% prediction interval using the degrees of freedom derived from the training data (number of samples minus number of features minus 1). This value will be used in the prediction function to construct the interval.

**Reasoning**:
To calculate the critical t-value, I need to import the `t` distribution from `scipy.stats`, calculate the degrees of freedom based on the training data, and then use the `t.ppf` function with the appropriate confidence level and degrees of freedom.
"""

from scipy.stats import t

# Calculate degrees of freedom (df)
n = X_train.shape[0]  # Number of samples
p = X_train.shape[1]  # Number of features
df = n - p - 1

# Calculate the critical t-value for a 95% prediction interval (alpha = 0.05, two-tailed, so 0.975 percentile)
t_critical = t.ppf(0.975, df)

print(f"Degrees of freedom (df): {df}")
print(f"Critical t-value for 95% prediction interval: {t_critical}")

"""**Reasoning**:
To incorporate the prediction interval into the `predict_new_car_price` function, I need to first calculate the Root Mean Squared Error (RMSE) from the training data. This RMSE will serve as an estimate of the residual standard error (s) needed for the prediction interval formula. I will predict on the `X_train` data and compute the RMSE between the `y_train` and these predictions.


"""

y_train_pred = regresion_multi_var.predict(X_train)
rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))

print(f"RMSE on training data: {rmse_train}")

"""**Reasoning**:
Now that the `t_critical` value and `rmse_train` have been calculated, I need to update the `predict_new_car_price` function to incorporate these values and return a 95% prediction interval along with the point prediction. This involves inverse transforming the RMSE to the original price scale and then calculating the prediction bounds.


"""

def predict_new_car_price(car_data):
    """
    Predicts the price of a car based on its features using the trained linear regression model.
    Returns the point prediction and its 95% prediction interval.

    Args:
        car_data (dict): A dictionary containing the features of the car.
                         Example: {'Levy': '1399', 'Manufacturer': 'LEXUS', 'Model': 'RX 450',
                                   'Prod. year': 2010, 'Category': 'Jeep', 'Leather interior': 'Yes',
                                   'Fuel type': 'Hybrid', 'Engine volume': '3.5', 'Mileage': '186005 km',
                                   'Cylinders': 6.0, 'Gear box type': 'Automatic', 'Drive wheels': '4x4',
                                   'Doors': '04-May', 'Wheel': 'Left wheel', 'Color': 'Silver', 'Airbags': 12}

    Returns:
        tuple: A tuple containing the predicted price, lower bound of 95% PI, and upper bound of 95% PI.
    """

    # Convert input dictionary to a pandas DataFrame (single row)
    input_df = pd.DataFrame([car_data])

    # Apply the same preprocessing steps as the training data

    # 1. Handle 'Levy' column
    input_df['Levy'] = input_df['Levy'].astype(str)
    input_df['Levy'] = input_df['Levy'].str.replace('-', '0')
    input_df['Levy'] = input_df['Levy'].astype(float)

    # 2. Handle 'Mileage' column
    input_df['Mileage'] = input_df['Mileage'].astype(str)
    input_df['Mileage'] = input_df['Mileage'].str.replace(' km', '')
    input_df['Mileage'] = input_df['Mileage'].astype(float)

    # 3. Handle 'Doors' column
    input_df['Doors'] = input_df['Doors'].str.extract(r'(\d+)')[0].astype(int)

    # 4. Handle 'Engine volume' and create 'Turbo_status'
    input_df['Turbo_status'] = np.where(input_df['Engine volume'].astype(str).str.contains('Turbo', case=False, na=False), 'Turbo', 'No turbo')
    input_df['Engine volume'] = input_df['Engine volume'].astype(str).str.extract(r'(\d+\.?\d*)')[0].astype(float)

    # 5. One-hot encode categorical features using the fitted encoder
    categorical_cols = ['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color', 'Turbo_status']
    input_df['Prod. year'] = input_df['Prod. year'].astype(str)

    encoded_features = encoder.transform(input_df[categorical_cols])
    encoded_features_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols), index=input_df.index)

    # 6. Select numerical features and concatenate with encoded features
    numerical_cols = ['Levy', 'Engine volume', 'Mileage', 'Cylinders', 'Doors', 'Airbags']
    processed_input_df = pd.concat([input_df[numerical_cols], encoded_features_df], axis=1)

    # 7. Reindex to match the training data columns exactly and fill missing with 0
    processed_input_df = processed_input_df.reindex(columns=X.columns, fill_value=0)

    # 8. Scale all features using the fitted scaler
    scaled_features = scaler.transform(processed_input_df)
    scaled_features_df = pd.DataFrame(scaled_features, columns=X.columns, index=processed_input_df.index)

    # 9. Make prediction using the trained model (scaled output)
    scaled_prediction = regresion_multi_var.predict(scaled_features_df)

    # 10. Inverse transform the scaled prediction to get the original price scale
    original_price = y_scaler.inverse_transform(scaled_prediction.reshape(-1, 1))[0][0]

    # 11. Calculate the prediction interval
    # To get RMSE in original scale, we can apply inverse scaling to a range of scaled RMSE values
    # However, for a simple linear model, the standard deviation of residuals (RMSE)
    # in the original scale can be approximated by multiplying the scaled RMSE by the std dev of the target.
    # More robust: use y_scaler.scale_ to directly convert scaled RMSE to original scale
    rmse_original_scale = rmse_train * y_scaler.scale_[0]

    margin_of_error = t_critical * rmse_original_scale
    lower_bound = original_price - margin_of_error
    upper_bound = original_price + margin_of_error

    return original_price, lower_bound, upper_bound

print("The predict_new_car_price function has been updated to return the prediction interval.")

"""## Verify_Prediction_Interval_Calculation

### Subtask:
Test the modified `predict_new_car_price` function with a sample car data to ensure it correctly returns the predicted price along with its 95% confidence interval.

**Reasoning**:
To test the updated `predict_new_car_price` function, I will define a sample car data dictionary with realistic values from the dataset, call the function, and print the returned predicted price and its 95% prediction interval.
"""

sample_car_data = {
    'Levy': '750',
    'Manufacturer': 'HYUNDAI',
    'Model': 'Sonata',
    'Prod. year': 2012,
    'Category': 'Sedan',
    'Leather interior': 'Yes',
    'Fuel type': 'Petrol',
    'Engine volume': '2.4',
    'Mileage': '150000 km',
    'Cylinders': 4.0,
    'Gear box type': 'Automatic',
    'Drive wheels': 'Front',
    'Doors': '04-May',
    'Wheel': 'Left wheel',
    'Color': 'White',
    'Airbags': 12
}

# Call the predict_new_car_price function
predicted_price, lower_bound, upper_bound = predict_new_car_price(sample_car_data)

# Print the results
print(f"Predicted Car Price: ${predicted_price:,.2f}")
print(f"95% Prediction Interval: [${lower_bound:,.2f}, ${upper_bound:,.2f}]")



"""# Task
Modify the `predict_new_car_price` function to ensure that `original_price`, `lower_bound`, and `upper_bound` are always greater than or equal to zero. This will involve applying `max(0, value)` to these outputs.

## Cap_Predicted_Prices

### Subtask:
Modify the `predict_new_car_price` function to ensure that the `original_price`, `lower_bound`, and `upper_bound` of the prediction are always greater than or equal to zero. This will involve applying `max(0, value)` to these outputs.

**Reasoning**:
I need to update the `predict_new_car_price` function as instructed, applying `max(0, value)` to the `original_price`, `lower_bound`, and `upper_bound` variables before they are returned. This will ensure all predicted price values are non-negative.
"""

def predict_new_car_price(car_data):
    """
    Predicts the price of a car based on its features using the trained linear regression model.
    Returns the point prediction and its 95% prediction interval.

    Args:
        car_data (dict): A dictionary containing the features of the car.
                         Example: {'Levy': '1399', 'Manufacturer': 'LEXUS', 'Model': 'RX 450',
                                   'Prod. year': 2010, 'Category': 'Jeep', 'Leather interior': 'Yes',
                                   'Fuel type': 'Hybrid', 'Engine volume': '3.5', 'Mileage': '186005 km',
                                   'Cylinders': 6.0, 'Gear box type': 'Automatic', 'Drive wheels': '4x4',
                                   'Doors': '04-May', 'Wheel': 'Left wheel', 'Color': 'Silver', 'Airbags': 12}

    Returns:
        tuple: A tuple containing the predicted price, lower bound of 95% PI, and upper bound of 95% PI.
    """

    # Convert input dictionary to a pandas DataFrame (single row)
    input_df = pd.DataFrame([car_data])

    # Apply the same preprocessing steps as the training data

    # 1. Handle 'Levy' column
    input_df['Levy'] = input_df['Levy'].astype(str)
    input_df['Levy'] = input_df['Levy'].str.replace('-', '0')
    input_df['Levy'] = input_df['Levy'].astype(float)

    # 2. Handle 'Mileage' column
    input_df['Mileage'] = input_df['Mileage'].astype(str)
    input_df['Mileage'] = input_df['Mileage'].str.replace(' km', '')
    input_df['Mileage'] = input_df['Mileage'].astype(float)

    # 3. Handle 'Doors' column
    input_df['Doors'] = input_df['Doors'].str.extract(r'(\d+)')[0].astype(int)

    # 4. Handle 'Engine volume' and create 'Turbo_status'
    input_df['Turbo_status'] = np.where(input_df['Engine volume'].astype(str).str.contains('Turbo', case=False, na=False), 'Turbo', 'No turbo')
    input_df['Engine volume'] = input_df['Engine volume'].astype(str).str.extract(r'(\d+\.?\d*)')[0].astype(float)

    # 5. One-hot encode categorical features using the fitted encoder
    categorical_cols = ['Manufacturer', 'Model', 'Prod. year', 'Category', 'Leather interior', 'Fuel type', 'Gear box type', 'Drive wheels', 'Wheel', 'Color', 'Turbo_status']
    input_df['Prod. year'] = input_df['Prod. year'].astype(str)

    encoded_features = encoder.transform(input_df[categorical_cols])
    encoded_features_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols), index=input_df.index)

    # 6. Select numerical features and concatenate with encoded features
    numerical_cols = ['Levy', 'Engine volume', 'Mileage', 'Cylinders', 'Doors', 'Airbags']
    processed_input_df = pd.concat([input_df[numerical_cols], encoded_features_df], axis=1)

    # 7. Reindex to match the training data columns exactly and fill missing with 0
    processed_input_df = processed_input_df.reindex(columns=X.columns, fill_value=0)

    # 8. Scale all features using the fitted scaler
    scaled_features = scaler.transform(processed_input_df)
    scaled_features_df = pd.DataFrame(scaled_features, columns=X.columns, index=processed_input_df.index)

    # 9. Make prediction using the trained model (scaled output)
    scaled_prediction = regresion_multi_var.predict(scaled_features_df)

    # 10. Inverse transform the scaled prediction to get the original price scale
    original_price = y_scaler.inverse_transform(scaled_prediction.reshape(-1, 1))[0][0]

    # 11. Calculate the prediction interval
    rmse_original_scale = rmse_train * y_scaler.scale_[0]

    margin_of_error = t_critical * rmse_original_scale
    lower_bound = original_price - margin_of_error
    upper_bound = original_price + margin_of_error

    # Ensure prices are not negative
    original_price = max(0, original_price)
    lower_bound = max(0, lower_bound)
    upper_bound = max(0, upper_bound)

    return original_price, lower_bound, upper_bound

print("The predict_new_car_price function has been updated to cap predicted prices at zero.")

"""## Verify_Capped_Prices

### Subtask:
Test the updated `predict_new_car_price` function with sample car data to confirm that all returned price values (point prediction, lower bound, and upper bound) are non-negative.

## Summary:

### Data Analysis Key Findings
*   The `predict_new_car_price` function was successfully updated to ensure that the predicted `original_price`, `lower_bound`, and `upper_bound` are always greater than or equal to zero.
*   This was achieved by applying `max(0, value)` to these three output variables within the function.

### Insights or Next Steps
*   This modification prevents the prediction of unrealistic negative car prices, enhancing the model's practical utility and interpretability.
*   The updated function provides a more robust price prediction mechanism, particularly for lower-valued cars where the prediction interval might otherwise extend into negative territory.

# Task
Calculate the mean of the 'Price' column from the `df_raw_std_filtered` DataFrame to establish a minimum acceptable price for car predictions.

## Calculate_Minimum_Acceptable_Price

### Subtask:
Compute the mean of the 'Price' column from the `df_raw_std_filtered` DataFrame. This value will serve as the fallback minimum price, replacing any predicted price (or interval bound) that falls to zero or below after initial capping.

**Reasoning**:
To compute the mean of the 'Price' column, I will access the 'Price' column of the `df_raw_std_filtered` DataFrame and apply the `.mean()` method, storing the result in `mean_price`.
"""